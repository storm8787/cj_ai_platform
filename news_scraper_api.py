"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë© ìŠ¤í¬ë¦½íŠ¸ (ë„¤ì´ë²„ API ë²„ì „)
í‚¤ì›Œë“œ: ì¶©ì£¼ì‹œ
ìµœì‹  30ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ (OpenAI ì„ë² ë”© ê¸°ë°˜ ì¤‘ë³µ ì œê±°)
"""

import requests
import json
from datetime import datetime, timedelta
import time
import os
import re
import numpy as np
from openai import OpenAI

# ============================================
# OpenAI ì„ë² ë”© ê¸°ë°˜ ì¤‘ë³µ ì œê±°
# ============================================

def get_embeddings(texts, client):
    """
    OpenAI ì„ë² ë”© APIë¡œ í…ìŠ¤íŠ¸ ë²¡í„°í™”
    
    Parameters:
    -----------
    texts : list
        ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    client : OpenAI
        OpenAI í´ë¼ì´ì–¸íŠ¸
    
    Returns:
    --------
    tuple : (embeddings ë¦¬ìŠ¤íŠ¸, response ê°ì²´)
    """
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    
    embeddings = [item.embedding for item in response.data]
    return embeddings, response

def cosine_similarity(vec1, vec2):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def remove_duplicates_with_embedding(news_list, threshold=0.85):
    """
    OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•œ ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°
    
    Parameters:
    -----------
    news_list : list
        ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    threshold : float
        ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.85, 85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µ)
    
    Returns:
    --------
    list : ì¤‘ë³µ ì œê±°ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    if not news_list:
        return []
    
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv('OPENAI_API_KEY', '')
    if not api_key:
        print("âš ï¸ OPENAI_API_KEYê°€ ì—†ì–´ì„œ ê¸°ë³¸ ì¤‘ë³µ ì œê±° ì‚¬ìš©")
        return remove_duplicates_simple(news_list)
    
    try:
        client = OpenAI(api_key=api_key)
        
        # ëª¨ë“  ì œëª© ì¶”ì¶œ
        titles = [news['title'] for news in news_list]
        
        print(f"ğŸ” {len(titles)}ê°œ ë‰´ìŠ¤ ì œëª© ì„ë² ë”© ì¤‘...")
        
        # ì„ë² ë”© ìƒì„±
        embeddings, response = get_embeddings(titles, client)
        
        # API ì‚¬ìš©ëŸ‰ ë¡œê¹…
        try:
            from openai_usage_logger import log_openai_usage
            log_openai_usage(response, model="text-embedding-3-small", request_type="news_embedding")
            print("ğŸ“Š ì„ë² ë”© ì‚¬ìš©ëŸ‰ ë¡œê¹… ì™„ë£Œ")
        except Exception as log_error:
            print(f"âš ï¸ ì‚¬ìš©ëŸ‰ ë¡œê¹… ì‹¤íŒ¨ (ê¸°ëŠ¥ì€ ì •ìƒ): {log_error}")
        
        # ì¤‘ë³µ ì œê±°
        unique_news = []
        unique_embeddings = []
        
        for i, news in enumerate(news_list):
            is_duplicate = False
            
            # ê¸°ì¡´ ë‰´ìŠ¤ë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
            for j, existing_emb in enumerate(unique_embeddings):
                similarity = cosine_similarity(embeddings[i], existing_emb)
                if similarity >= threshold:
                    print(f"â­ï¸ ì¤‘ë³µ ì œê±° (ìœ ì‚¬ë„ {similarity:.2f}): {news['title'][:30]}...")
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_news.append(news)
                unique_embeddings.append(embeddings[i])
        
        print(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(news_list)}ê°œ â†’ {len(unique_news)}ê°œ")
        return unique_news
    
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ì¤‘ë³µ ì œê±° ì‹¤íŒ¨: {e}")
        print("âš ï¸ ê¸°ë³¸ ì¤‘ë³µ ì œê±°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return remove_duplicates_simple(news_list)

def remove_duplicates_simple(news_list, threshold=0.7):
    """
    ê°„ë‹¨í•œ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (í´ë°±ìš©)
    """
    from difflib import SequenceMatcher
    
    unique_news = []
    seen_titles = []
    
    for news in news_list:
        is_duplicate = False
        for seen_title in seen_titles:
            similarity = SequenceMatcher(None, news['title'], seen_title).ratio()
            if similarity >= threshold:
                is_duplicate = True
                break
        
        if not is_duplicate:
            unique_news.append(news)
            seen_titles.append(news['title'])
    
    return unique_news

# ============================================
# ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘
# ============================================

def scrape_naver_news(keyword="ì¶©ì£¼ì‹œ", max_results=30):
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
    
    Parameters:
    -----------
    keyword : str
        ê²€ìƒ‰ í‚¤ì›Œë“œ (ê¸°ë³¸ê°’: ì¶©ì£¼ì‹œ)
    max_results : int
        ìˆ˜ì§‘í•  ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸ê°’: 30)
    
    Returns:
    --------
    list : ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    client_id = os.getenv('NAVER_CLIENT_ID', '')
    client_secret = os.getenv('NAVER_CLIENT_SECRET', '')
    
    if not client_id or not client_secret:
        print("âŒ ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRET í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return []
    
    news_list = []
    
    # ë„¤ì´ë²„ ê²€ìƒ‰ API URL
    url = "https://openapi.naver.com/v1/search/news.json"
    
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    
    print(f"ğŸ” '{keyword}' ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘...")
    
    try:
        # APIëŠ” í•œ ë²ˆì— ìµœëŒ€ 100ê°œê¹Œì§€ ê°€ëŠ¥
        # ì¤‘ë³µ ì œê±° ê³ ë ¤í•´ì„œ 2ë°° ìš”ì²­
        display = min(100, max_results * 2)
        
        params = {
            "query": keyword,
            "display": display,
            "start": 1,
            "sort": "date"  # ìµœì‹ ìˆœ
        }
        
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        
        data = response.json()
        items = data.get('items', [])
        
        print(f"ğŸ“¥ APIì—ì„œ {len(items)}ê°œ ë‰´ìŠ¤ ë°›ìŒ")
        
        for idx, item in enumerate(items):
            try:
                # HTML íƒœê·¸ ì œê±°
                title = remove_html_tags(item.get('title', ''))
                description = remove_html_tags(item.get('description', ''))
                
                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                pub_date = item.get('pubDate', '')
                formatted_date = format_date(pub_date)
                
                news_data = {
                    'id': idx + 1,
                    'title': title,
                    'link': item.get('originallink', item.get('link', '')),
                    'press': extract_press_name(item.get('link', '')),
                    'date': formatted_date,
                    'summary': description[:150] + "..." if len(description) > 150 else description,
                    'content': description,
                    'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                news_list.append(news_data)
                
            except Exception as e:
                print(f"âŒ ë‰´ìŠ¤ í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"ğŸ“° {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ë¨, ì¤‘ë³µ ì œê±° ì‹œì‘...")
        
        # OpenAI ì„ë² ë”© ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        unique_news = remove_duplicates_with_embedding(news_list, threshold=0.85)
        
        # max_results ê°œìˆ˜ë¡œ ì œí•œ
        unique_news = unique_news[:max_results]
        
        # ID ì¬ì •ë ¬
        for i, news in enumerate(unique_news):
            news['id'] = i + 1
        
        print(f"\nâœ… ìµœì¢… {len(unique_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")
        return unique_news
    
    except requests.exceptions.RequestException as e:
        print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def remove_html_tags(text):
    """HTML íƒœê·¸ ì œê±° ë° ì—”í‹°í‹° ë””ì½”ë”©"""
    import html as html_module
    # 1. HTML íƒœê·¸ ì œê±°
    clean = re.compile('<.*?>')
    text = re.sub(clean, '', text)
    # 2. HTML ì—”í‹°í‹° ë””ì½”ë”© (&quot; â†’ ", &amp; â†’ &)
    text = html_module.unescape(text)
    return text

def extract_press_name(link):
    """
    URLì—ì„œ ì–¸ë¡ ì‚¬ ì´ë¦„ ì¶”ì¶œ
    """
    if 'n.news.naver.com' in link:
        parts = link.split('/')
        if len(parts) > 5:
            return parts[5]
    
    from urllib.parse import urlparse
    domain = urlparse(link).netloc
    
    press_mapping = {
        'chosun.com': 'ì¡°ì„ ì¼ë³´',
        'donga.com': 'ë™ì•„ì¼ë³´',
        'joongang.co.kr': 'ì¤‘ì•™ì¼ë³´',
        'joins.com': 'ì¤‘ì•™ì¼ë³´',
        'hani.co.kr': 'í•œê²¨ë ˆ',
        'khan.co.kr': 'ê²½í–¥ì‹ ë¬¸',
        'hankyung.com': 'í•œêµ­ê²½ì œ',
        'mk.co.kr': 'ë§¤ì¼ê²½ì œ',
        'yonhapnews.co.kr': 'ì—°í•©ë‰´ìŠ¤',
        'yna.co.kr': 'ì—°í•©ë‰´ìŠ¤',
        'kbs.co.kr': 'KBS',
        'sbs.co.kr': 'SBS',
        'mbc.co.kr': 'MBC',
        'jtbc.co.kr': 'JTBC',
        'news1.kr': 'ë‰´ìŠ¤1',
        'newsis.com': 'ë‰´ì‹œìŠ¤',
        'edaily.co.kr': 'ì´ë°ì¼ë¦¬',
        'mt.co.kr': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
        'inews24.com': 'ì•„ì´ë‰´ìŠ¤24',
        'zdnet.co.kr': 'ZDNet',
        'asiae.co.kr': 'ì•„ì‹œì•„ê²½ì œ',
        'sedaily.com': 'ì„œìš¸ê²½ì œ',
        'fnnews.com': 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',
        'etnews.com': 'ì „ìì‹ ë¬¸',
        'cjilbo.com': 'ì¶©ë¶ì¼ë³´',
        'cbinews.co.kr': 'ì¶©ë¶ì¸ë‰´ìŠ¤',
        'ggilbo.com': 'ê¸ˆê°•ì¼ë³´',
        'dynews.co.kr': 'ëŒ€ì „ì¼ë³´',
        'daejonilbo.com': 'ëŒ€ì „ì¼ë³´',
        'cctoday.co.kr': 'ì¶©ì²­íˆ¬ë°ì´',
        'cctimes.kr': 'ì¶©ì²­íƒ€ì„ì¦ˆ',
        'chungnamilbo.com': 'ì¶©ë‚¨ì¼ë³´',
        'joongdo.co.kr': 'ì¤‘ë„ì¼ë³´',
        'jamill.kr' : 'ì¤‘ì•™ë§¤ì¼',
        'dailycc.net' : 'ì¶©ì²­ì‹ ë¬¸',
        'dynews.co.kr' : 'ë™ì–‘ì¼ë³´',
        'jbnews.com' : 'ì¤‘ë¶€ë§¤ì¼',        
    }
    
    for key, value in press_mapping.items():
        if key in domain:
            return value
    
    return domain.replace('www.', '').split('.')[0]

def format_date(pub_date):
    """
    ë„¤ì´ë²„ API ë‚ ì§œ í˜•ì‹ ë³€í™˜
    """
    try:
        dt = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %z')
        return dt.strftime('%Y-%m-%d %H:%M')
    except:
        return pub_date

# ============================================
# ì €ì¥ í•¨ìˆ˜
# ============================================

def save_to_json(news_list, filepath='data/news_data.json'):
    """
    ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
    """
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    kst_time = datetime.now() + timedelta(hours=9)
    
    data = {
        'keyword': 'ì¶©ì£¼ì‹œ',
        'total_count': len(news_list),
        'last_updated': kst_time.strftime('%Y-%m-%d %H:%M:%S'),
        'news': news_list
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ {filepath}ì— ì €ì¥ ì™„ë£Œ!")
    
    return data

def save_to_gist(news_list):
    """
    ë‰´ìŠ¤ ë°ì´í„°ë¥¼ GitHub Gistì— ì €ì¥
    """
    gist_token = os.getenv('GIST_TOKEN', '')
    gist_id = os.getenv('GIST_ID', '')
    
    if not gist_token:
        print("âš ï¸ GIST_TOKENì´ ì—†ì–´ì„œ Gist ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    
    kst_time = datetime.now() + timedelta(hours=9)
    
    data = {
        'keyword': 'ì¶©ì£¼ì‹œ',
        'total_count': len(news_list),
        'last_updated': kst_time.strftime('%Y-%m-%d %H:%M:%S'),
        'news': news_list
    }
    
    headers = {
        'Authorization': f'token {gist_token}',
        'Accept': 'application/vnd.github.v3+json'
    }
    
    gist_data = {
        'description': 'ì¶©ì£¼ì‹œ ë‰´ìŠ¤ ë°ì´í„° (ìë™ ì—…ë°ì´íŠ¸)',
        'public': False,
        'files': {
            'news_data.json': {
                'content': json.dumps(data, ensure_ascii=False, indent=2)
            }
        }
    }
    
    try:
        if gist_id:
            url = f'https://api.github.com/gists/{gist_id}'
            response = requests.patch(url, headers=headers, json=gist_data)
        else:
            url = 'https://api.github.com/gists'
            response = requests.post(url, headers=headers, json=gist_data)
        
        response.raise_for_status()
        result = response.json()
        
        gist_id = result['id']
        raw_url = result['files']['news_data.json']['raw_url']
        
        print(f"\nâ˜ï¸ Gist ì €ì¥ ì™„ë£Œ!")
        print(f"   Gist ID: {gist_id}")
        print(f"   Raw URL: {raw_url}")
        
        return raw_url
    
    except Exception as e:
        print(f"âŒ Gist ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

# ============================================
# ë©”ì¸ ì‹¤í–‰
# ============================================

if __name__ == "__main__":
    # ë‰´ìŠ¤ ìŠ¤í¬ë© ì‹¤í–‰
    news_data = scrape_naver_news(keyword="ì¶©ì£¼ì‹œ", max_results=30)
    
    if news_data:
        # ë¡œì»¬ JSON íŒŒì¼ë¡œ ì €ì¥ (í…ŒìŠ¤íŠ¸/ë°±ì—…ìš©)
        save_to_json(news_data)
        
        # GitHub Gistì— ì €ì¥ (GIST_TOKENì´ ìˆëŠ” ê²½ìš°)
        save_to_gist(news_data)
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë¯¸ë¦¬ë³´ê¸°:")
        for news in news_data[:5]:
            print(f"\nì œëª©: {news['title']}")
            print(f"ì–¸ë¡ ì‚¬: {news['press']}")
            print(f"ë‚ ì§œ: {news['date']}")
            print(f"ë§í¬: {news['link'][:50]}...")
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")